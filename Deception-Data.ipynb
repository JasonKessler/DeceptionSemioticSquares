{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scattetext to Examine Deceptive Writing\n",
    "## Jason Kessler\n",
    "## PSL Talk, Dec 14 2018\n",
    "\n",
    "Link to Scattertext documentaion: https://github.com/jasonkessler/scattertext\n",
    "\n",
    "### Data\n",
    "\n",
    "From: http://web.eecs.umich.edu/~mihalcea/downloads.html\n",
    "\n",
    "*Open-Domain Deception*\n",
    "\n",
    "This is a crowdsourced deception dataset consisting of short open domain truths and lies from 512 users. Seven lies and seven truths are provided for each user. The dataset also includes user's demographic information, such as gender, age, country of origin, and education level. download (August 27, 2015)\n",
    "\n",
    "Veronica Perez-Rosas and Rada Mihalcea, Experiments in Open Domain Deception Detection, in Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2015), Lisbon, Portugal, September 2015.\n",
    "\n",
    "*Real-life Deception*\n",
    "\n",
    "A multimodal dataset consisting of real-life deception: deceptive and truthful trial testimonies, manually transcribed and annotated. The dataset includes 121 short videos, along with their transcriptions and gesture annotations. download (June 15, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import codecs\n",
    "import io\n",
    "import csv\n",
    "import scattertext as st\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://web.eecs.umich.edu/~mihalcea/downloads/openDeception.2015.tar.gz')\n",
    "tarball = tarfile.open(mode=\"r:gz\", fileobj = io.BytesIO(r.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with oddly formatted csv\n",
    "raw = tarball.extractfile('OpenDeception/7Truth7LiesDataset.csv')\n",
    "data = []\n",
    "for i, line in enumerate(raw.read().decode('utf8').split('\\n')[1:]):\n",
    "    row = list(csv.reader(io.StringIO(line)))\n",
    "    if(len(row)) == 0: continue\n",
    "    row = row[0]\n",
    "    d = {}\n",
    "    d['id'] = row[0]\n",
    "    d['gender'] = row[1]\n",
    "    d['age'] = row[2]\n",
    "    nexti = 4\n",
    "    if row[3][0] == \"'\":\n",
    "        edu = []\n",
    "        for r in row[3:]:\n",
    "            edu.append(r)\n",
    "            if r[-1] == \"'\": break\n",
    "            nexti += 1\n",
    "        d['education'] = ','.join(edu)[1:-1]\n",
    "    else:\n",
    "        d['education'] = ','.join(row[3])\n",
    "    d['country'] = row[nexti]\n",
    "    d['text'] = '\\n'.join(row[nexti+1:-1])\n",
    "    d['class'] = row[-1]\n",
    "    data.append(d)\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonkessler/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: invalid escape sequence '\\%'\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['parse'] = df.text.apply(lambda x: st.whitespace_nlp_with_sentences(codecs.escape_decode(bytes(x, \"utf-8\"))[0].decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = st.CorpusFromParsedDocuments(df, category_col='class', parsed_col='parse').build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(corpus):\n",
    "    return corpus.get_df().apply(lambda x: x['gender'] + ', ' + str(x['age']) + ', ' + x['country'] + '; ' + x['education'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Male', 'Female'], ['lie', 'truth'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.recategorize(corpus.get_df()['gender']).get_categories(), corpus.get_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1600\"\n",
       "            height=\"700\"\n",
       "            src=\"lie_vs_truth.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x119a90d30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = st.produce_scattertext_explorer(corpus, \n",
    "                                       category='lie', \n",
    "                                       term_scorer=st.RankDifference(), \n",
    "                                       transform=st.Scalers.percentile_dense,\n",
    "                                       metadata=get_metadata(corpus))\n",
    "file_name = 'lie_vs_truth.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width = 1600, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1600\"\n",
       "            height=\"700\"\n",
       "            src=\"female_vs_male.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1162c17f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_corpus = corpus.recategorize(df['gender'])\n",
    "html = st.produce_scattertext_explorer(gender_corpus, \n",
    "                                       category='Female', \n",
    "                                       not_categories=['Male'],\n",
    "                                       term_scorer=st.RankDifference(), \n",
    "                                       transform=st.Scalers.percentile_dense,\n",
    "                                       metadata=get_metadata(gender_corpus))\n",
    "file_name = 'female_vs_male.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width = 1600, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['gender'] + ' ' + df['class']\n",
    "four_square_corpus = corpus.recategorize(df['category']).get_unigram_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_square = st.FourSquare(\n",
    "    four_square_corpus,\n",
    "    category_a_list=['Female truth'],\n",
    "    category_b_list=['Female lie'],\n",
    "    not_category_a_list=['Male lie'],\n",
    "    not_category_b_list=['Male truth'],\n",
    "    scorer=st.RankDifference(),\n",
    "    labels={'a': 'Female-Specific Truth',\n",
    "            'b': 'Female-Specific Lie',\n",
    "            'not_a_and_not_b': 'Male',\n",
    "            'a_and_b': 'Female',\n",
    "            'a_and_not_b': 'Truth',\n",
    "            'b_and_not_a': 'Lie',\n",
    "            'not_a': 'Male-Specific Lie',\n",
    "            'not_b': 'Male-Specific Truth',\n",
    "            }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_df = four_square_corpus.get_df()\n",
    "#meta = display_df['id'] + ', ' + display_df['country'] + ', Age: ' + display_df['age']\n",
    "html = st.produce_four_square_explorer(four_square=four_square,\n",
    "                                       x_label='Truth-Lie',\n",
    "                                       y_label='Male-Female',\n",
    "                                       use_full_doc=True,\n",
    "                                       minimum_term_frequency=2,\n",
    "                                       metadata=get_metadata(four_square_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1600\"\n",
       "            height=\"700\"\n",
       "            src=\"lie_vs_truth_four_square.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1162c15f8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'lie_vs_truth_four_square.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width = 1600, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://web.eecs.umich.edu/~mihalcea/downloads/RealLifeDeceptionDetection.2016.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "annotations_df = pd.read_csv(zf.open('Real-life_Deception_Detection_2016/Annotation/All_Gestures_Deceptive and Truthful.csv'))\n",
    "annotations_df['id'] = annotations_df['id'].apply(lambda x: x.split('.')[0])\n",
    "court_df = pd.DataFrame([{'filename': fn,  'id': fn.split('/')[-1].split('.')[0], 'text': zf.open(fn).read()} \n",
    "                         for fn in zf.namelist()if fn.endswith('.txt') and not fn.startswith('__') and not fn.endswith('README.txt')])\n",
    "court_df['parse'] = court_df.text.apply(lambda x: st.whitespace_nlp_with_sentences(x.decode('utf8')))\n",
    "court_df['class'] = court_df['id'].apply(lambda x: 'lie' if '_lie_' in x else 'truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1600\"\n",
       "            height=\"700\"\n",
       "            src=\"court_scattertplot.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11ea902e8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "court_corpus = st.CorpusFromParsedDocuments(court_df, category_col='class', parsed_col='parse').build().get_unigram_corpus()\n",
    "html = st.produce_scattertext_explorer(court_corpus, \n",
    "                                       category='truth', \n",
    "                                       not_categories=['lie'],\n",
    "                                       term_scorer=st.RankDifference(), \n",
    "                                       transform=st.Scalers.percentile_dense)\n",
    "file_name = 'court_scattertplot.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width = 1600, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonkessler/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2961: DtypeWarning: Columns (63,108,109,110,176) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "general_inquirer = st.FeatsFromGeneralInquirer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1600\"\n",
       "            height=\"700\"\n",
       "            src=\"gi_court_scattertplot.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11bee3ac8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gi_court_corpus = st.CorpusFromParsedDocuments(court_df, \n",
    "                                               category_col='class', \n",
    "                                               parsed_col='parse', \n",
    "                                               feats_from_spacy_doc=general_inquirer).build().get_unigram_corpus()\n",
    "html = st.produce_frequency_explorer(gi_court_corpus, \n",
    "                                       category='truth', \n",
    "                                       not_categories=['lie'],\n",
    "                                       term_scorer=st.RankDifference(), \n",
    "                                       use_non_text_features=True,\n",
    "                                       use_full_doc=True,\n",
    "                                       topic_model_term_lists=general_inquirer.get_top_model_term_lists(),\n",
    "                                       metadata_descriptions=general_inquirer.get_definitions(),\n",
    "                                       grey_threshold=0)\n",
    "file_name = 'gi_court_scattertplot.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width = 1600, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_square_corpus = st.CorpusFromParsedDocuments(court_df, \n",
    "                                                  category_col='class', \n",
    "                                                  parsed_col='parse', \n",
    "                                                  feats_from_spacy_doc=feat_builder).build().get_unigram_corpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "mturk_df = df[df.countryfmt.isin(['usa', 'unitedstates', 'unitedstatesofamerica', 'us', 'america'])]\n",
    "joint_df = pd.concat([court_df.assign(real='courtroom'),  mturk_df.assign(real='mturk')], sort=True)\n",
    "joint_df['class'] = joint_df['class'].replace({'lie': 'deceptive', 'truth': 'truthful'})\n",
    "joint_df = joint_df.loc[joint_df['parse'].dropna().index]\n",
    "joint_df['category'] = joint_df['class'] + ' ' + joint_df['real']="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_square_corpus = st.CorpusFromParsedDocuments(joint_df, category_col='category', parsed_col='parse').build().get_unigram_corpus()\n",
    "four_square = st.FourSquare(\n",
    "    four_square_corpus,\n",
    "    category_a_list=['deceptive mturk'],\n",
    "    category_b_list=['deceptive courtroom'],\n",
    "    not_category_a_list=['truthful courtroom'],\n",
    "    not_category_b_list=['truthful mturk'],\n",
    "    scorer=st.RankDifference(),\n",
    "    labels={'a': '',\n",
    "            'b': '',\"\"\n",
    "            'not_a_and_not_b': 'Truth',\n",
    "            'a_and_b': 'Lie',\n",
    "            'a_and_not_b': 'MTurk',\n",
    "            'b_and_not_a': 'Court Room',\n",
    "            'not_a': '',\n",
    "            'not_b': '',\n",
    "            }\n",
    ")\n",
    "display_df = four_square_corpus.get_df()\n",
    "meta = display_df['category']\n",
    "html = st.produce_four_square_explorer(four_square=four_square,\n",
    "                                       x_label='Mturk-Court',\n",
    "                                       y_label='Truth-Lie',\n",
    "                                       use_full_doc=True,\n",
    "                                       minimum_term_frequency=2,\n",
    "                                       censor_points=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1600\"\n",
       "            height=\"700\"\n",
       "            src=\"mturk_vs_courth_four_square.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x126b39cf8>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'mturk_vs_courth_four_square.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width = 1600, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
